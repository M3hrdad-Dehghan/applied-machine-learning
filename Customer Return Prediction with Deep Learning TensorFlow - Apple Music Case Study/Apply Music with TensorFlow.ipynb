{"cells":[{"cell_type":"markdown","metadata":{"id":"IYJdPj5nah_6"},"source":["# **Apple Music Business Case**"]},{"cell_type":"markdown","source":["## **1-What is Problem?**"],"metadata":{"id":"Uihr5wmtaqUR"}},{"cell_type":"markdown","metadata":{"id":"Qoji0yQEah_9"},"source":["We have a dataset from Apple music Application users. Each user in the database has made a purchase at least once, that's why he/she is in the database. We want to create a machine learning algorithm based on our available data that can predict if a user will buy again from the Apple company.\n","\n","The main idea is that if a user has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts SOLELY on users that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities."]},{"cell_type":"markdown","source":["## **2-Familiar with Data**"],"metadata":{"id":"ReqrBVribNks"}},{"cell_type":"markdown","source":["\n","We have a .csv summarizing the data. There are several variables:\n","* Customer ID\n","* overall_music_length\n","* average_muci_length\n","* overall_music_price\n","* average_music_price\n","* has_review\n","* review\n","* support_ticket\n","* tenure\n","* target"],"metadata":{"id":"iq4QGd7gbNuR"}},{"cell_type":"markdown","source":["## **3-Data Preprocessing**"],"metadata":{"id":"9v-h7emPcn99"}},{"cell_type":"markdown","metadata":{"id":"4BcIivFuah_9"},"source":["Since we are dealing with real life data, we will need to preprocess it a bit."]},{"cell_type":"markdown","metadata":{"id":"ja_aUfgVah_-"},"source":["### **3.1-Extract the data from the csv**"]},{"cell_type":"markdown","source":["Here we first load data with `Numpy` package."],"metadata":{"id":"cXDNopuAdVZk"}},{"cell_type":"code","source":["import numpy as np\n","\n","PATH = '/content/drive/MyDrive/TensorFlow Project/apple_music.csv'\n","\n","raw_csv_data = np.loadtxt(PATH,\n","                          delimiter = ',',\n","                          skiprows = 1)"],"metadata":{"id":"MXFXCqg2dWSW","executionInfo":{"status":"ok","timestamp":1768855023882,"user_tz":300,"elapsed":20,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["The inputs are all columns in the csv, except for the first one which is `user id`."],"metadata":{"id":"S-jrh5xoez7p"}},{"cell_type":"code","source":["unscaled_inputs_all = raw_csv_data[: , 1:-1]\n","unscaled_inputs_all"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gv-AlNvfeVOT","executionInfo":{"status":"ok","timestamp":1768855023883,"user_tz":300,"elapsed":18,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}},"outputId":"6a58e490-962a-48fc-b542-47323fad69cb"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1620.  , 1620.  ,   19.73, ...,   10.  ,    5.  ,   92.  ],\n","       [2160.  , 2160.  ,    5.33, ...,    8.91,    0.  ,    0.  ],\n","       [2160.  , 2160.  ,    5.33, ...,    8.91,    0.  ,  388.  ],\n","       ...,\n","       [2160.  , 2160.  ,    6.14, ...,    8.91,    0.  ,    0.  ],\n","       [1620.  , 1620.  ,    5.33, ...,    8.  ,    0.  ,   90.  ],\n","       [1674.  , 3348.  ,    5.33, ...,    8.91,    0.  ,    0.  ]])"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["The targets are in the last column. That's how datasets are conventionally organized."],"metadata":{"id":"FZocOwtvfK0i"}},{"cell_type":"code","source":["targets_all = raw_csv_data[: , -1]\n","targets_all"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBRMONL8eVUT","executionInfo":{"status":"ok","timestamp":1768855023895,"user_tz":300,"elapsed":20,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}},"outputId":"1dd13f75-a76d-447c-87d1-7b49718c0d53"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., ..., 0., 0., 1.])"]},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["### **3.2-Data Shuffling**"],"metadata":{"id":"m3B5XcXYnQ22"}},{"cell_type":"markdown","source":["When the data was collected it was actually arranged by date.\n","Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n","Since we will be batching, we want the data to be as randomly spread out as possible.\n","\n","The we use the shuffled indices to shuffle the inputs and targets."],"metadata":{"id":"25zrYV3knbKl"}},{"cell_type":"code","source":["shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n","np.random.shuffle(shuffled_indices)\n","\n","shuffled_inputs = unscaled_inputs_all[shuffled_indices]\n","shuffled_targets = targets_all[shuffled_indices]"],"metadata":{"id":"qxfLjalImCrO","executionInfo":{"status":"ok","timestamp":1768855266602,"user_tz":300,"elapsed":18,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["### **3.3-Data Splitting**"],"metadata":{"id":"QyOdF259nu4U"}},{"cell_type":"markdown","source":["In this step, we need to split data into three parts. These are the steps we walk-through:\n","1. Count the total number of samples\n","2. Count the samples in each subset, assuming we want 80-10-10 distribution of training, validation, and test.\n","3. The 'test' dataset contains all remaining data.\n","4. Create variables that record the inputs and targets for training\n","5. Create variables that record the inputs and targets for validation.\n","6. Create variables that record the inputs and targets for test.\n"],"metadata":{"id":"gqKcoqFfn37D"}},{"cell_type":"code","source":["# Step 1\n","samples_count = shuffled_inputs.shape[0]\n","\n","# Step 2\n","train_samples_count = int(0.8 * samples_count)\n","validation_samples_count = int(0.1 * samples_count)\n","\n","# Step 3\n","test_samples_count = samples_count - train_samples_count - validation_samples_count\n","\n","# Step 4\n","train_inputs = shuffled_inputs[:train_samples_count]\n","train_targets = shuffled_targets[:train_samples_count]\n","\n","# Step 5\n","validation_inputs = shuffled_inputs[train_samples_count : train_samples_count + validation_samples_count]\n","validation_targets = shuffled_targets[train_samples_count : train_samples_count + validation_samples_count]\n","\n","# Step 6\n","test_inputs = shuffled_inputs[train_samples_count + validation_samples_count:]\n","test_targets = shuffled_targets[train_samples_count + validation_samples_count:]"],"metadata":{"id":"bpMsJIdumCn1","executionInfo":{"status":"ok","timestamp":1768855273647,"user_tz":300,"elapsed":14,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["print(f\"training target is: {np.sum(train_targets)}, num rows is: {train_samples_count}, and % is: {np.sum(train_targets) / train_samples_count}\")\n","print(f\"val target is: {np.sum(validation_targets)}, num rows is: {validation_samples_count}, and % is: {np.sum(validation_targets) / validation_samples_count}\")\n","print(f\"test target is: {np.sum(test_targets)}, num rows is: {test_samples_count}, and % is: {np.sum(test_targets) / test_samples_count}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PI0aAopnxjt","executionInfo":{"status":"ok","timestamp":1768855277308,"user_tz":300,"elapsed":45,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}},"outputId":"3456c74e-0d75-4671-c4e0-ed5d84ba238a"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["training target is: 1785.0, num rows is: 11267, and % is: 0.15842726546551877\n","val target is: 235.0, num rows is: 1408, and % is: 0.1669034090909091\n","test target is: 217.0, num rows is: 1409, and % is: 0.1540099361249113\n"]}]},{"cell_type":"markdown","source":["### **3.4-Data Preprocessing**"],"metadata":{"id":"i0jTM5gQlWN5"}},{"cell_type":"markdown","source":["That's the only place we use sklearn functionality. We will take advantage of its preprocessing capabilities. It's a simple line of code, which standardizes the inputs, as we explained in one of the lectures.\n"],"metadata":{"id":"NcbN2TnBl0t1"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","train_inputs = scaler.fit_transform(train_inputs)\n","validation_inputs = scaler.transform(validation_inputs)\n","test_inputs = scaler.transform(test_inputs)"],"metadata":{"id":"EA3sOyqIi1rL","executionInfo":{"status":"ok","timestamp":1768855291201,"user_tz":300,"elapsed":6,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["## **4-Building Model**"],"metadata":{"id":"OJTqngFKu15W"}},{"cell_type":"markdown","source":["### **4.1- Set Hyperparameters and Construct Model**"],"metadata":{"id":"27x1UxLMzvHi"}},{"cell_type":"markdown","source":["We must import the TensorFlow library"],"metadata":{"id":"cRljx9C8u-2J"}},{"cell_type":"code","source":["import tensorflow as tf"],"metadata":{"id":"tMoBvtWEnxgd","executionInfo":{"status":"ok","timestamp":1768855298930,"user_tz":300,"elapsed":7,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["First of all, we set some hyperparameters, such as input size, output size and hidden layer to define width and depth of the model."],"metadata":{"id":"tCrOMCmgyF-n"}},{"cell_type":"code","source":["INPUT_SIZE = 10\n","OUTPUT_SIZE = 1\n","HIDDEN_LAYER_SIZE = 50\n","DROPOUT_RATE = 0.3"],"metadata":{"id":"XEgzKqNlwrl4","executionInfo":{"status":"ok","timestamp":1768855302535,"user_tz":300,"elapsed":16,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["Then, we should define the sequence of model's steps."],"metadata":{"id":"9gsffrJGyrjA"}},{"cell_type":"code","source":["from tensorflow.keras import regularizers\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n","\n","    tf.keras.layers.Dropout(DROPOUT_RATE),\n","\n","    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),\n","\n","    tf.keras.layers.Dropout(DROPOUT_RATE),\n","\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n"],"metadata":{"id":"7S5NJhZXnzkY","executionInfo":{"status":"ok","timestamp":1768855306387,"user_tz":300,"elapsed":11,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["In the next step, we set the optimizer, loss function and metric."],"metadata":{"id":"yZPeapwrzfBJ"}},{"cell_type":"code","source":["model.compile(optimizer = 'adam',\n","                            loss = 'binary_crossentropy',\n","                            metrics = ['accuracy'])"],"metadata":{"id":"KpvVlWEay1RP","executionInfo":{"status":"ok","timestamp":1768855319327,"user_tz":300,"elapsed":15,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["In the next step, we set number of batches. Because we don't want to update weights after each epoch and instead we use `mini-batch` method to update weight several times during one epoch."],"metadata":{"id":"Fg0Pbq8h0ABh"}},{"cell_type":"code","source":["BATCH_SIZE = 100\n","\n","EPOCH = 200"],"metadata":{"id":"fcTcT564y1Nq","executionInfo":{"status":"ok","timestamp":1768855322310,"user_tz":300,"elapsed":18,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":["We also set early stopping to prevenet overfitting with patience 2."],"metadata":{"id":"vVCm9-ve0SmU"}},{"cell_type":"code","source":["PATIENCE = 2\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","    monitor = \"val_loss\",\n","    patience = PATIENCE,\n","    restore_best_weights = True\n",")"],"metadata":{"id":"5IwiD1id0XiB","executionInfo":{"status":"ok","timestamp":1768855324626,"user_tz":300,"elapsed":16,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}}},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":["### **4.2- Train the Model**"],"metadata":{"id":"EwaFysSBz6b4"}},{"cell_type":"markdown","source":["Now, all is set and we are ready to fit and train the model. In this step, before fitting, since we have imbalanced data, we should define weight for each lable in trainign set to allow model knows this imbalance."],"metadata":{"id":"_tD-tqT80jsA"}},{"cell_type":"code","source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","classes = np.unique(train_targets)\n","weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=train_targets)\n","class_weight = dict(zip(classes, weights))\n","\n","model.fit(\n","    train_inputs,\n","    train_targets,\n","    validation_data=(validation_inputs, validation_targets),\n","    class_weight=class_weight,\n","          epochs=200, batch_size=100,\n","          callbacks=[early_stopping],\n","          verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"whG2Q6gP_lNy","executionInfo":{"status":"ok","timestamp":1768855384288,"user_tz":300,"elapsed":2160,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}},"outputId":"329f8926-1dda-47bc-e890-55952e2a46f1"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","113/113 - 1s - 11ms/step - accuracy: 0.7041 - loss: 0.6484 - val_accuracy: 0.7528 - val_loss: 0.5762\n","Epoch 2/200\n","113/113 - 0s - 2ms/step - accuracy: 0.7258 - loss: 0.6102 - val_accuracy: 0.8111 - val_loss: 0.5384\n","Epoch 3/200\n","113/113 - 0s - 2ms/step - accuracy: 0.7482 - loss: 0.6011 - val_accuracy: 0.8224 - val_loss: 0.5383\n","Epoch 4/200\n","113/113 - 0s - 2ms/step - accuracy: 0.7661 - loss: 0.5930 - val_accuracy: 0.8260 - val_loss: 0.5433\n","Epoch 5/200\n","113/113 - 0s - 2ms/step - accuracy: 0.7792 - loss: 0.5912 - val_accuracy: 0.7884 - val_loss: 0.5730\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7cb660264a70>"]},"metadata":{},"execution_count":80}]},{"cell_type":"markdown","source":["## **5-Testing Model**"],"metadata":{"id":"_TdBBEJS3BuD"}},{"cell_type":"markdown","source":["After training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n","\n","It is very important to realize that fiddling with the hyperparameters overfits the validation dataset."],"metadata":{"id":"zkyW-QVq3GQq"}},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)\n","\n","print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dgS043ixO5E","executionInfo":{"status":"ok","timestamp":1768855459718,"user_tz":300,"elapsed":96,"user":{"displayName":"Mehrdad M. Dehghan","userId":"08222177611817016206"}},"outputId":"785b2d04-5788-4440-af6a-2da2bc7ecb61"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8479 - loss: 0.5337 \n","\n","Test loss: 0.54. Test accuracy: 83.39%\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}